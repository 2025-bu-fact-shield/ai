import os
import pandas as pd
import torch
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer, AutoConfig, get_scheduler
from transformers.models.electra.modeling_electra import ElectraForSequenceClassification
from tqdm import tqdm
from torch.cuda.amp import autocast, GradScaler

# ì„¤ì •
MODEL_NAME = "beomi/KcELECTRA-base"
EPOCHS = 4
BATCH_SIZE = 64
LEARNING_RATE = 2e-5
MAX_LEN = 64
WARMUP_STEPS = 100
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# CSV ë¡œë”© í•¨ìˆ˜ (ë‹¨ì¼ íŒŒì¼ìš©)
def load_csv_data(csv_path):
    try:
        df = pd.read_csv(
            csv_path,
            usecols=["newsTitle", "clickbaitClass"],
            dtype={"newsTitle": str, "clickbaitClass": int}
        ).dropna()
        return df
    except Exception as e:
        print(f"âŒ {csv_path} ë¡œë”© ì‹¤íŒ¨: {e}")
        return pd.DataFrame()

# Dataset í´ë˜ìŠ¤
class NewsDataset(Dataset):
    def __init__(self, dataframe, tokenizer):
        self.samples = dataframe
        self.tokenizer = tokenizer

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        row = self.samples.iloc[idx]
        encoding = self.tokenizer(
            row["newsTitle"],
            truncation=True,
            padding="max_length",
            max_length=MAX_LEN,
            return_tensors="pt"
        )
        return {
            "input_ids": encoding["input_ids"].squeeze(),
            "attention_mask": encoding["attention_mask"].squeeze(),
            "label": torch.tensor(row["clickbaitClass"])
        }

# ê²€ì¦ í•¨ìˆ˜
def evaluate(model, val_loader, device):
    model.eval()
    total_loss = 0
    correct = 0
    total = 0
    with torch.no_grad():
        for batch in val_loader:
            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)
            labels = batch["label"].to(device)

            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
            loss = outputs.loss
            logits = outputs.logits

            total_loss += loss.item()
            preds = torch.argmax(logits, dim=1)
            correct += (preds == labels).sum().item()
            total += labels.size(0)

    avg_loss = total_loss / len(val_loader)
    accuracy = correct / total
    return avg_loss, accuracy

# ì‚¬ìš©ì ì •ì˜ ëª¨ë¸ í´ë˜ìŠ¤
class CustomElectra(ElectraForSequenceClassification):
    def __init__(self, config):
        super().__init__(config)
        self.classifier.dropout.p = 0.3

# ë©”ì¸ ì‹¤í–‰
def main():
    print(f"\U0001F680 í˜„ì¬ ì¥ì¹˜: {DEVICE}")
    if DEVICE.type == 'cuda':
        print(f"\U0001F5A5ï¸ CUDA ë””ë°”ì´ìŠ¤ ì´ë¦„: {torch.cuda.get_device_name(0)}")

    train_path = "/content/drive/MyDrive/training_clickbait.csv"
    val_path = "/content/drive/MyDrive/validation_clickbait.csv"

    train_df = load_csv_data(train_path)
    val_df = load_csv_data(val_path)
    print(f"\U0001F4CA í•™ìŠµ ìƒ˜í”Œ ìˆ˜: {len(train_df)}, ê²€ì¦ ìƒ˜í”Œ ìˆ˜: {len(val_df)}")

    if len(train_df) == 0 or len(val_df) == 0:
        print("â— ë°ì´í„°ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. í•™ìŠµì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
        return

    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    train_dataset = NewsDataset(train_df, tokenizer)
    val_dataset = NewsDataset(val_df, tokenizer)

    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)
    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)

    config = AutoConfig.from_pretrained(MODEL_NAME)
    config.num_labels = 2
    model = CustomElectra.from_pretrained(MODEL_NAME, config=config)
    model.to(DEVICE)

    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)
    lr_scheduler = get_scheduler(
        name="linear",
        optimizer=optimizer,
        num_warmup_steps=WARMUP_STEPS,
        num_training_steps=EPOCHS * len(train_loader)
    )
    scaler = GradScaler()

    best_accuracy = 0
    for epoch in range(EPOCHS):
        model.train()
        total_loss = 0.0
        print(f"\n================== Epoch {epoch+1}/{EPOCHS} ==================")
        for batch in tqdm(train_loader, desc=f"[{epoch+1}ì—í­] í•™ìŠµ ì¤‘"):
            input_ids = batch["input_ids"].to(DEVICE)
            attention_mask = batch["attention_mask"].to(DEVICE)
            labels = batch["label"].to(DEVICE)

            optimizer.zero_grad()
            with autocast():
                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
                loss = outputs.loss

            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()
            lr_scheduler.step()
            total_loss += loss.item()

        avg_train_loss = total_loss / len(train_loader)
        val_loss, val_accuracy = evaluate(model, val_loader, DEVICE)

        print(f"âœ… Epoch {epoch+1} ê²°ê³¼:")
        print(f"   - í‰ê·  í•™ìŠµ ì†ì‹¤: {avg_train_loss:.4f}")
        print(f"   - ê²€ì¦ ì†ì‹¤: {val_loss:.4f}")
        print(f"   - ê²€ì¦ ì •í™•ë„: {val_accuracy:.4f}")

        if val_accuracy > best_accuracy:
            best_accuracy = val_accuracy
            torch.save(model.state_dict(), "/content/drive/MyDrive/kcelectra_best_model.pt")
            print(f"ğŸ“¥ ëª¨ë¸ ì €ì¥ë¨ (ì •í™•ë„ í–¥ìƒ): {best_accuracy:.4f}")

    print(f"ğŸ ìµœì¢… ìµœê³  ê²€ì¦ ì •í™•ë„: {best_accuracy:.4f}")

if __name__ == "__main__":
    main()
