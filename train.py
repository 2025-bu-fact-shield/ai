import os
import pandas as pd
import torch
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer, AutoConfig, AutoModelForSequenceClassification, get_scheduler
from tqdm import tqdm
from torch.cuda.amp import autocast, GradScaler
import matplotlib.pyplot as plt

# ì„¤ì •
MODEL_NAME = "beomi/KcELECTRA-base-v2022"  # ğŸ” ì—¬ê¸°ì„œ Electra ë˜ëŠ” KoBERT êµì²´ ê°€ëŠ¥
EPOCHS = 2
BATCH_SIZE = 32
LEARNING_RATE = 2e-5
MAX_LEN = 32
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

train_losses = []
val_losses = []
val_accuracies = []
# ì—í­ë³„ ê¸°ë¡ ì €ì¥ì„ ìœ„í•œ ë¦¬ìŠ¤íŠ¸

# CSV ë¡œë”© í•¨ìˆ˜
def load_csv_data(csv_folder):
    all_data = []
    for fname in os.listdir(csv_folder):
        if fname.endswith(".csv"):
            path = os.path.join(csv_folder, fname)
            try:
                df = pd.read_csv(
                    path,
                    usecols=["newsTitle", "clickbaitClass"],
                    dtype={"newsTitle": str, "clickbaitClass": int}
                ).dropna()
                all_data.append(df)
            except Exception as e:
                print(f"âŒ {fname} ë¡œë”© ì‹¤íŒ¨: {e}")
    return pd.concat(all_data, ignore_index=True)

# Dataset í´ë˜ìŠ¤
class NewsDataset(Dataset):
    def __init__(self, dataframe, tokenizer):
        self.samples = dataframe
        self.tokenizer = tokenizer

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        row = self.samples.iloc[idx]
        encoding = self.tokenizer(
            row["newsTitle"],
            truncation=True,
            padding="max_length",
            max_length=MAX_LEN,
            return_tensors="pt"
        )
        return {
            "input_ids": encoding["input_ids"].squeeze(),
            "attention_mask": encoding["attention_mask"].squeeze(),
            "label": torch.tensor(row["clickbaitClass"])
        }

# ê²€ì¦ í•¨ìˆ˜
def evaluate(model, val_loader, device):
    model.eval()
    total_loss = 0
    correct = 0
    total = 0
    with torch.no_grad():
        for batch in val_loader:
            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)
            labels = batch["label"].to(device)

            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
            loss = outputs.loss
            logits = outputs.logits

            total_loss += loss.item()
            preds = torch.argmax(logits, dim=1)
            correct += (preds == labels).sum().item()
            total += labels.size(0)

    avg_loss = total_loss / len(val_loader)
    accuracy = correct / total
    return avg_loss, accuracy

# ë©”ì¸ ì‹¤í–‰
def main():
    print(f"ğŸš€ í˜„ì¬ ì¥ì¹˜: {DEVICE}")
    if DEVICE.type == 'cuda':
        print(f"ğŸ–¥ï¸ CUDA ë””ë°”ì´ìŠ¤ ì´ë¦„: {torch.cuda.get_device_name(0)}")

    train_path = "./DataSets/training"
    val_path = "./DataSets/validation"

    train_df = load_csv_data(train_path)
    # ğŸ”» í•™ìŠµ ë°ì´í„° 40%
    train_df = train_df.sample(frac=1.0, random_state=42).reset_index(drop=True)  
    val_df = load_csv_data(val_path)
    # ğŸ”» ê²€ì¦ ë°ì´í„° 40%
    val_df = val_df.sample(frac=1.0, random_state=42).reset_index(drop=True)  

    print(f"ğŸ“Š í•™ìŠµ ìƒ˜í”Œ ìˆ˜: {len(train_df)}, ê²€ì¦ ìƒ˜í”Œ ìˆ˜: {len(val_df)}")

    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    train_dataset = NewsDataset(train_df, tokenizer)
    val_dataset = NewsDataset(val_df, tokenizer)

    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)
    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)

    config = AutoConfig.from_pretrained(MODEL_NAME)
    config.num_labels = 2
    model = AutoModelForSequenceClassification.from_pretrained(
        MODEL_NAME, 
        num_labels=2)
    model.to(DEVICE)

    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)
    lr_scheduler = get_scheduler(
        name="linear", optimizer=optimizer, num_warmup_steps=0,
        num_training_steps=EPOCHS * len(train_loader)
    )
    scaler = GradScaler()

    for epoch in range(EPOCHS):
        model.train()
        total_loss = 0.0
        print(f"\n================== Epoch {epoch+1}/{EPOCHS} ==================")
        for batch in tqdm(train_loader, desc=f"[{epoch+1}ì—í­] í•™ìŠµ ì¤‘"):
            input_ids = batch["input_ids"].to(DEVICE)
            attention_mask = batch["attention_mask"].to(DEVICE)
            labels = batch["label"].to(DEVICE)

            optimizer.zero_grad()
            with autocast():
                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
                loss = outputs.loss

            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()
            lr_scheduler.step()
            total_loss += loss.item()

        avg_train_loss = total_loss / len(train_loader)
        val_loss, val_accuracy = evaluate(model, val_loader, DEVICE)

        train_losses.append(avg_train_loss)
        val_losses.append(val_loss)
        val_accuracies.append(val_accuracy)

        print(f"âœ… Epoch {epoch+1} ê²°ê³¼:")
        print(f"   - í‰ê·  í•™ìŠµ ì†ì‹¤: {avg_train_loss:.4f}")
        print(f"   - ê²€ì¦ ì†ì‹¤: {val_loss:.4f}")
        print(f"   - ê²€ì¦ ì •í™•ë„: {val_accuracy:.4f}")

    save_path = "./kcelectra_model_dropout1.0.pt"  # ë˜ëŠ” ì ˆëŒ€ ê²½ë¡œ ì§€ì •
    os.makedirs(os.path.dirname(save_path), exist_ok=True)  # ê²½ë¡œ ë³´ì¥
    torch.save(model.state_dict(), save_path)
    print(f"ğŸ‰ ì „ì²´ í•™ìŠµ ì™„ë£Œ! ëª¨ë¸ ì €ì¥ë¨: {save_path}")

    epochs = range(1, EPOCHS + 1)
    plt.figure(figsize=(12, 5))
    plt.subplot(1, 2, 1)
    plt.plot(epochs, train_losses, marker='o', label='Train Loss')
    plt.plot(epochs, val_losses, marker='s', label='Validation Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Epoch-wise Loss')
    plt.legend()

    # ê²€ì¦ ì •í™•ë„
    plt.subplot(1, 2, 2)
    plt.plot(epochs, val_accuracies, marker='^', color='green', label='Validation Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.title('Epoch-wise Validation Accuracy')
    plt.ylim(0, 1)
    plt.legend()

    plt.tight_layout()
    plt.savefig("training_results1.0.png")
    plt.show()

    print("ğŸ“ˆ í•™ìŠµ ê²°ê³¼ ì‹œê°í™” ì™„ë£Œ! 'training_results1.0.png'ë¡œ ì €ì¥ë¨.")

if __name__ == "__main__":
    main()
